{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import inspect\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk import download as nltk_download\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk_download(\"punkt\")\n",
    "nltk_download(\"stopwords\")\n",
    "nltk_download(\"wordnet\")\n",
    "nltk_download(\"averaged_perceptron_tagger\")\n",
    "nltk_download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directories\n",
    "BASE_DIR: Path = Path().resolve().parent\n",
    "DATA_DIR: Path = BASE_DIR / \"data\"\n",
    "\n",
    "# Load data\n",
    "df: pd.DataFrame = pd.read_csv(\n",
    "    DATA_DIR / \"intermediate\" / \"25072024_reviews_openrefine.csv\"\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Processing text: comparing spaCy and NLTK\n",
    "\n",
    "Text data must be processed before it can be used in machine learning models. This processing includes tokenization, lemmatization, and removing stopwords and punctuation. Here we will try out two popular libraries for text processing: spaCy and NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. NLTK\n",
    "\n",
    "NLTK (natural language toolkit) is an extensive library for a wide range of NLP tasks. Its the older of the two, first released in 2001. It contains a lot of functionality, but requires more manual work to get started.\n",
    "\n",
    "Use t-SNE for visualization (and try different parameters to get something visually pleasing!), but rather do not run clustering afterwards, in particular do not use distance- or density based algorithms, as this information was intentionally (!) lost. Neighborhood-graph based approaches may be fine, but then you don't need to first run t-SNE beforehand, just use the neighbors immediately (because t-SNE tries to keep this nn-graph largely intact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text: str, processor: WordNetLemmatizer | PorterStemmer) -> str:\n",
    "    \"\"\"Preparing text for TF-IDF vectorization.\"\"\"\n",
    "\n",
    "    # Convert to lowercase and remove non-alphabetic characters\n",
    "    processed_text: str = re.sub(r\"[^a-z]\", \" \", text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens: list[str] = word_tokenize(processed_text)\n",
    "\n",
    "    # POS tagging. Nltk POS tags are based solely on token, without taking into account the context.\n",
    "    tagged_tokens: list[tuple[Any, str]] = pos_tag(tokens)\n",
    "\n",
    "    # Apply the processor (lemmatizer or stemmer). Lemmatizer is better than stemmer.\n",
    "    # get_wordnet_tag to map the treebank pos tags to wordnet tags\n",
    "    if isinstance(processor, WordNetLemmatizer):\n",
    "        processed_tokens = [\n",
    "            processor.lemmatize(token, pos=get_wordnet_tag(tag))\n",
    "            for token, tag in tagged_tokens\n",
    "        ]\n",
    "    elif isinstance(processor, PorterStemmer):\n",
    "        processed_tokens = [processor.stem(token) for token in tokens]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Processor must be an instance of either WordNetLemmatizer or PorterStemmer\"\n",
    "        )\n",
    "    # Join the processed tokens back into a single string and return\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "def get_wordnet_tag(treebank_tag) -> str:\n",
    "    \"\"\"\n",
    "    Transform treebank pos tags to wordnet tags for WordNet lemmatizer.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply the text processing function to the \"blind_assessment\" column\n",
    "prepared_text: pd.Series = df[\"blind_assessment\"].apply(\n",
    "    prepare_text, processor=lemmatizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF to the prepared text\n",
    "# ==================================================#\n",
    "\n",
    "vectorizer: TfidfVectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix: csr_matrix = vectorizer.fit_transform(\n",
    "    prepared_text\n",
    ")  # -> scipy sparse matrix\n",
    "\n",
    "feature_names: list[str] = vectorizer.get_feature_names_out().tolist()\n",
    "tfidf_df: pd.DataFrame = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(tfidf_matrix.toarray().shape)\n",
    "\n",
    "# Check that the number of rows matches\n",
    "assert tfidf_matrix.shape[0] == df.shape[0]\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud of top keywords\n",
    "# ================================== #\n",
    "def plot_wordcloud(\n",
    "    word_scores: dict[str, float], fout: Path, title: str | None = None, **kwargs\n",
    "):\n",
    "    \"\"\"Plotting a wordcloud from a dictionary of word scores.\"\"\"\n",
    "\n",
    "    valid_args = inspect.signature(WordCloud).parameters\n",
    "    for key in kwargs:\n",
    "        if key not in valid_args:\n",
    "            raise ValueError(f\"Invalid keyword argument: {key}\")\n",
    "\n",
    "    wordcloud = WordCloud(**kwargs)\n",
    "    wordcloud.generate_from_frequencies(word_scores)\n",
    "\n",
    "    plt.figure(figsize=(20, 10), dpi=300)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title, fontsize=20)\n",
    "    plt.savefig(fout, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "output_path: Path = BASE_DIR / \"imgs\" / \"blind_assessment_tfidf_wordcloud.png\"\n",
    "tfidf_scores: np.ndarray = tfidf_matrix.mean(axis=0).A1\n",
    "\n",
    "word_scores: dict[str, float] = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "plot_wordcloud(\n",
    "    word_scores,\n",
    "    fout=output_path,\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    max_font_size=200,\n",
    "    max_words=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply spacy to prepared_text\n",
    "# ============================ #\n",
    "\n",
    "docs = list(nlp.pipe(df[\"blind_assessment\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_text = []\n",
    "\n",
    "for doc in docs:\n",
    "    joined_tokens = \" \".join(\n",
    "        [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    )\n",
    "    prepared_text.append(joined_tokens.lower())\n",
    "\n",
    "\n",
    "# Checking we didnt lose any rows\n",
    "assert len(prepared_text) == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_text = pd.Series(prepared_text)\n",
    "\n",
    "# Apply TF-IDF vectorization to cleaned_assessment column\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(prepared_text)  # -> scipy sparse matrix\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "print(tfidf_matrix.toarray().shape)\n",
    "# Check that the number of rows match\n",
    "assert tfidf_matrix.shape[0] == df.shape[0]\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = BASE_DIR / \"imgs\" / \"blind_assessment_tfidf_wordcloud_spacy.png\"\n",
    "tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
    "word_scores: dict[str, float] = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "plot_wordcloud(\n",
    "    word_scores,\n",
    "    fout=output_path,\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    max_font_size=200,\n",
    "    max_words=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We will two clustering algorithms: Kmeans and DBSCAN. Each will run on the original tf-ifd vectors and on PCA reduced vectors.\n",
    "\n",
    "Clusters will be visualized in 2D using t-SNE. Note that t-SNE should not be used for dimensionality reduction and only for producing visualizations.\n",
    "\n",
    "### PCA\n",
    "\n",
    "Dimensionality reduction can be helpful for clustering methods as they are typically sensitive to the *curse of dimensionality*.\n",
    "\n",
    "\n",
    "### Kmeans\n",
    "\n",
    "Kmeans is a simple clustering algorithm that partitions the data into k clusters where k must be specified. To determine the optimal number of clusters, we will use the elbow method.\n",
    "This method consists of plotting the sum of squared distances between each point and the centroid of the cluster it belongs to. The optimal number of clusters is the point where the curve starts to flatten out. With the optimal k value, we will run Kmeans on the original tf-idf vectors and on PCA reduced data. \n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm that groups together points that are closely packed together. It is able to find clusters of arbitrary shapes and sizes. DBSCAN has two parameters: epsilon and min_samples. Epsilon is the maximum distance between two samples for one to be considered as in the neighborhood of the other. Min_samples is the number of samples in a neighborhood for a point to be considered as a core point. Epsilon is the most important parameter to tune. Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection\n",
    "\n",
    "### HDBSCAN\n",
    "\n",
    "HDBSCAN is a hierarchical version of DBSCAN. It is able to find clusters of varying densities and sizes. It has two parameters: min_cluster_size and min_samples. Min_cluster_size is the smallest size grouping that should be considered a cluster. Min_samples is the number of samples in a neighborhood for a point to be considered as a core point. HDBSCAN is much more than scale invariant though – it is capable of multi-scale clustering, which accounts for clusters with varying density. Traditional DBSCAN assumes that any potential clusters are homogeneous in density. HDBSCAN is free from such constraints.\n",
    "\n",
    "### t-SNE\n",
    "t-SNE is a manifold learning method for visualizing high-dimensional data. It maps the high-dimensional data to a 2D or 3D space so that the data points that are close in the high-dimensional space remain close in the low-dimensional space. t-SNE is generally not recommended for dimensionality reduction as it does not preserve global structure and distances.\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/\n",
    "\n",
    "\n",
    "### UMAP\n",
    "\n",
    "Notes on umap, tf-ifd for text analysiss and visulization\n",
    "\n",
    "https://umap-learn.readthedocs.io/en/latest/sparse.html\n",
    "\n",
    "### Can I cluster the results of UMAP?\n",
    "\n",
    "This is hard to answer well, but essentially the answer is “yes, with care”. To start with it matters what clustering algorithm you are going to use. Since UMAP does not necessarily produce clean spherical clusters something like K-Means is a poor choice. I would recommend HDBSCAN or similar. The catch here is that UMAP, with its uniform density assumption, does not preserve density well. What UMAP will do, however, is contract connected components of the manifold together. Providing you have enough data for UMAP to distinguish that information then you can get useful clustering results out since algorithms like HDBSCAN will easily pick out the components after applying UMAP.\n",
    "\n",
    "UMAP does offer significant improvements over algorithms like t-SNE for clustering. First, by preserving more global structure and creating meaningful separation between connected components of the manifold on which the data lies, UMAP offers more meaningful clusters. Second, because it supports arbitrary embedding dimensions, UMAP allows embedding to larger dimensional spaces that make it more amenable to clustering.\n",
    "\n",
    "Consider a typical pipeline: high-dimensional embedding (300+) => PCA to reduce to 50 dimensions => UMAP to reduce to 10-20 dimensions => HDBSCAN for clustering / some plain algorithm for classification;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data -> pre-process -> tf-idf vectors\n",
    "\n",
    "Then... \n",
    "\n",
    "Kmeans (silhouette scores) -> t-SNE, UMAP\n",
    "\n",
    "PCA -> Kmeans (silhouette scores) -> t-SNE, UMAP\n",
    "\n",
    "PCA -> HDBSCAN -> t-SNE, UMAP\n",
    "\n",
    "PCA -> UMAP -> HDBSCAN -> t-SNE, UMAP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df - read in data\n",
    "# tfidf_matrix - matrix of tfidf values\n",
    "# vectorizer - object returned by TfidfVectorizer that contains the feature names and scores\n",
    "\n",
    "# Kmeans clustering\n",
    "X = tfidf_matrix.toarray()\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# plot explained variance ratiio\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"cumulative explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 95% of variance\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) > 0.95)\n",
    "print(f\"Number of components to keep: {n_components}\")\n",
    "\n",
    "# keep the first n_components from our pca\n",
    "X_pca = X_pca[:, :n_components]\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use silhouette score to determine the number of clusters\n",
    "silhouette_scores = []\n",
    "K = range(30, 100)\n",
    "\n",
    "for k in tqdm(K):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "    cluster_labels = kmeans.predict(X_pca)\n",
    "    silhouette = silhouette_score(X_pca, cluster_labels)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# number of clusters with the highest silhouette score\n",
    "best_k = K[np.argmax(silhouette_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    x=K,\n",
    "    y=silhouette_scores,\n",
    "    height=400,\n",
    "    width=800,\n",
    "    labels={\"x\": \"Number of Clusters\", \"y\": \"Silhouette Score\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters with the highest silhouette score\n",
    "best_k = K[np.argmax(silhouette_scores)]\n",
    "\n",
    "kmeanModel = KMeans(n_clusters=best_k).fit(X_pca)\n",
    "cluster_labels = kmeanModel.predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE on the PCA features to visualize the clusters\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=5000, random_state=42)\n",
    "tsne_features = tsne.fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne_features, columns=[\"component1\", \"component2\"])\n",
    "\n",
    "tsne_df[\"cluster\"] = cluster_labels\n",
    "tsne_df[\"cluster\"] = tsne_df[\"cluster\"].astype(str)\n",
    "tsne_df[\"country\"] = df[\"origin_country\"]\n",
    "tsne_df[\"rating\"] = df[\"rating\"]\n",
    "\n",
    "fig = px.scatter(\n",
    "    tsne_df,\n",
    "    x=\"component1\",\n",
    "    y=\"component2\",\n",
    "    color=\"cluster\",\n",
    "    # hover shows country and rating\n",
    "    hover_data={\"cluster\": True, \"country\": True, \"rating\": True},\n",
    "    height=800,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans\n",
    "# Kmeans after PCA\n",
    "# DBSCAN - fine tune the hyperparameters\n",
    "# DBSCAN with cosine metric\n",
    "\n",
    "# Plot the clusters using t-SNE, and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Visualization\n",
    "t-SNE\n",
    "PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
