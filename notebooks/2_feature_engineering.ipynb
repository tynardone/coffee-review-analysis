{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tylernardone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tylernardone/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tylernardone/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tylernardone/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / \"intermediate\" / \"25072024_reviews_openrefine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text_with_lemmatization(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenize and convert to lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Remove adverbs, verbs, and stopwords from pos_tags\n",
    "    pos_tags_filtered = [(word, pos) for word, pos in pos_tags if not (pos.startswith('V') or pos.startswith('R'))\n",
    "                                                                       and word not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # Lemmatize words based on POS tags\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_tag(pos) or 'n')  # Default to noun if no tag\n",
    "        for word, pos in pos_tags_filtered]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "def get_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_assessment'] = df['blind_assessment'].apply(preprocess_text_with_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_assessment</th>\n",
       "      <th>blind_assessment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rich intricate layered lemon zest cacao nib vi...</td>\n",
       "      <td>Rich, intricate and layered. Lemon zest, roast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sweet tart crisply herbaceous chocolate green ...</td>\n",
       "      <td>Gently sweet-tart, crisply herbaceous. Baking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>floral tropical leaning magnolia green banana ...</td>\n",
       "      <td>Floral-toned, tropical-leaning. Magnolia, guav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>floral complex flower lilac cacoa nib tangerin...</td>\n",
       "      <td>Crisply floral, delicately lively. Complex flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>espresso complex dark chocolate molasses narci...</td>\n",
       "      <td>Evaluated as espresso. Intrigungly complex, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tart crisply fruity dark chocolate magnolia ar...</td>\n",
       "      <td>Tart-leaning, crisply fruity. Pomegranate, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gentle sweet coffee balance understated depth ...</td>\n",
       "      <td>A gentle, sweet coffee whose balance and under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sweet savory chocolaty spicy chocolate plum ro...</td>\n",
       "      <td>Sweet/savory, chocolaty. Spicy chocolate, plum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>delicate bright juicy precious ripe lemon oran...</td>\n",
       "      <td>Delicate, bright, juicy, precious. Ripe lemon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lush overripe harrar fruit note salty bitter t...</td>\n",
       "      <td>Lush, sweetly overripe Harrar fruit notes are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cleaned_assessment  \\\n",
       "0  rich intricate layered lemon zest cacao nib vi...   \n",
       "1  sweet tart crisply herbaceous chocolate green ...   \n",
       "2  floral tropical leaning magnolia green banana ...   \n",
       "3  floral complex flower lilac cacoa nib tangerin...   \n",
       "4  espresso complex dark chocolate molasses narci...   \n",
       "5  tart crisply fruity dark chocolate magnolia ar...   \n",
       "6  gentle sweet coffee balance understated depth ...   \n",
       "7  sweet savory chocolaty spicy chocolate plum ro...   \n",
       "8  delicate bright juicy precious ripe lemon oran...   \n",
       "9  lush overripe harrar fruit note salty bitter t...   \n",
       "\n",
       "                                    blind_assessment  \n",
       "0  Rich, intricate and layered. Lemon zest, roast...  \n",
       "1  Gently sweet-tart, crisply herbaceous. Baking ...  \n",
       "2  Floral-toned, tropical-leaning. Magnolia, guav...  \n",
       "3  Crisply floral, delicately lively. Complex flo...  \n",
       "4  Evaluated as espresso. Intrigungly complex, ba...  \n",
       "5  Tart-leaning, crisply fruity. Pomegranate, dar...  \n",
       "6  A gentle, sweet coffee whose balance and under...  \n",
       "7  Sweet/savory, chocolaty. Spicy chocolate, plum...  \n",
       "8  Delicate, bright, juicy, precious. Ripe lemon,...  \n",
       "9  Lush, sweetly overripe Harrar fruit notes are ...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_assessment', 'blind_assessment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=200)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_assessment'])\n",
    "keywords = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acidity' 'acidy' 'almond' 'apple' 'apricot' 'aroma' 'aromatic'\n",
      " 'aromatics' 'astringency' 'astringent' 'baker' 'balance' 'balanced'\n",
      " 'banana' 'bergamot' 'berry' 'big' 'bit' 'bitter' 'bittersweet' 'black'\n",
      " 'blackberry' 'blossom' 'blueberry' 'body' 'brandy' 'bright' 'brisk'\n",
      " 'brittle' 'brown' 'buoyant' 'butter' 'buttery' 'cacao' 'caramel' 'carry'\n",
      " 'cashew' 'cedar' 'center' 'character' 'cherry' 'chocolate' 'chocolaty'\n",
      " 'cinnamon' 'citrus' 'citrusy' 'clean' 'cocoa' 'coffee' 'complex'\n",
      " 'complexity' 'consolidates' 'continued' 'creamy' 'crisp' 'crisply' 'cup'\n",
      " 'currant' 'cut' 'dark' 'date' 'deep' 'deeply' 'delicate' 'distinct'\n",
      " 'dried' 'dry' 'earth' 'edge' 'espresso' 'fine' 'finish' 'fir' 'flavor'\n",
      " 'floral' 'flower' 'freesia' 'fresh' 'fruit' 'fruity' 'fudge' 'gardenia'\n",
      " 'gentle' 'grape' 'grapefruit' 'green' 'guava' 'hazelnut' 'heavy' 'herb'\n",
      " 'high' 'hint' 'honey' 'honeysuckle' 'intense' 'jasmine' 'juicy' 'ken'\n",
      " 'lavender' 'lead' 'lean' 'lemon' 'light' 'like' 'lilac' 'lime' 'long'\n",
      " 'low' 'lush' 'lychee' 'magnolia' 'mango' 'maple' 'marjoram' 'medium'\n",
      " 'mild' 'milk' 'molasses' 'mouthfeel' 'musk' 'myrrh' 'narcissus' 'nib'\n",
      " 'night' 'note' 'nut' 'oak' 'orange' 'ounce' 'part' 'particular' 'peach'\n",
      " 'pear' 'peppercorn' 'pineapple' 'pink' 'pipe' 'pistachio' 'plum' 'plush'\n",
      " 'pomegranate' 'powder' 'profile' 'pungent' 'quiet' 'raisin' 'raspberry'\n",
      " 'red' 'resonant' 'rich' 'richly' 'ripe' 'roast' 'roasty' 'round'\n",
      " 'sandalwood' 'satiny' 'savory' 'semi' 'short' 'silky' 'simple' 'single'\n",
      " 'slight' 'small' 'smoky' 'smooth' 'soft' 'spice' 'spicy' 'star'\n",
      " 'strawberry' 'structure' 'sugar' 'suggestion' 'sweet' 'sweetness' 'syrup'\n",
      " 'syrupy' 'tamarind' 'tangerine' 'tart' 'tea' 'thyme' 'tobacco' 'toffee'\n",
      " 'tone' 'toned' 'undertone' 'vanilla' 'velvety' 'verbena' 'vibrant'\n",
      " 'violet' 'viscous' 'walnut' 'wine' 'wisteria' 'wood' 'zest']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acidity</th>\n",
       "      <th>almond</th>\n",
       "      <th>apple</th>\n",
       "      <th>apricot</th>\n",
       "      <th>aroma</th>\n",
       "      <th>aromatic</th>\n",
       "      <th>bake</th>\n",
       "      <th>baker</th>\n",
       "      <th>balance</th>\n",
       "      <th>balanced</th>\n",
       "      <th>...</th>\n",
       "      <th>toned</th>\n",
       "      <th>turn</th>\n",
       "      <th>undertone</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>velvety</th>\n",
       "      <th>verbena</th>\n",
       "      <th>vibrant</th>\n",
       "      <th>wine</th>\n",
       "      <th>wood</th>\n",
       "      <th>zest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>0.087311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>0.103188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264733</td>\n",
       "      <td>0.084010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>0.102313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>0.086661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>0.113697</td>\n",
       "      <td>0.263962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7563 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acidity    almond  apple   apricot     aroma  aromatic      bake  \\\n",
       "0     0.117075  0.000000    0.0  0.000000  0.095316       0.0  0.000000   \n",
       "1     0.072021  0.000000    0.0  0.000000  0.058635       0.0  0.197761   \n",
       "2     0.125894  0.000000    0.0  0.000000  0.102495       0.0  0.000000   \n",
       "3     0.088734  0.000000    0.0  0.000000  0.072242       0.0  0.000000   \n",
       "4     0.000000  0.000000    0.0  0.000000  0.064100       0.0  0.000000   \n",
       "...        ...       ...    ...       ...       ...       ...       ...   \n",
       "7558  0.087311  0.000000    0.0  0.000000  0.071083       0.0  0.000000   \n",
       "7559  0.103188  0.000000    0.0  0.264733  0.084010       0.0  0.000000   \n",
       "7560  0.102313  0.000000    0.0  0.000000  0.000000       0.0  0.000000   \n",
       "7561  0.086661  0.000000    0.0  0.000000  0.070554       0.0  0.000000   \n",
       "7562  0.113697  0.263962    0.0  0.000000  0.092565       0.0  0.000000   \n",
       "\n",
       "      baker   balance  balanced  ...  toned  turn  undertone   vanilla  \\\n",
       "0       0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "1       0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "2       0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "3       0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "4       0.0  0.147886       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "...     ...       ...       ...  ...    ...   ...        ...       ...   \n",
       "7558    0.0  0.163996       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "7559    0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "7560    0.0  0.192176       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "7561    0.0  0.000000       0.0  ...    0.0   0.0        0.0  0.259586   \n",
       "7562    0.0  0.213558       0.0  ...    0.0   0.0        0.0  0.000000   \n",
       "\n",
       "       velvety   verbena   vibrant      wine  wood      zest  \n",
       "0     0.000000  0.000000  0.000000  0.000000   0.0  0.249149  \n",
       "1     0.000000  0.468924  0.000000  0.000000   0.0  0.000000  \n",
       "2     0.331839  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "3     0.000000  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "4     0.000000  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "...        ...       ...       ...       ...   ...       ...  \n",
       "7558  0.000000  0.000000  0.244272  0.000000   0.0  0.000000  \n",
       "7559  0.000000  0.000000  0.000000  0.000000   0.0  0.219596  \n",
       "7560  0.000000  0.000000  0.286245  0.000000   0.0  0.000000  \n",
       "7561  0.000000  0.000000  0.000000  0.544986   0.0  0.000000  \n",
       "7562  0.000000  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "\n",
       "[7563 rows x 150 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
