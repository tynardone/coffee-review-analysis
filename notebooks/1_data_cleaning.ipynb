{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table_of_contents'></a>\n",
    "\n",
    "0. [Import libraries](#imports)\n",
    "1. [Import data](#import_data)\n",
    "2. [Initial Cleaning](#initial_cleaning)\n",
    "3. [Price and Quantity Cleaning](#price_and_quantity_cleaning)\n",
    "4. [Data Checks](#data_checks)\n",
    "5. [Export cleaned data](#export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries <a id='imports'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, HBox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "mpl.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import raw data <a id='import_data'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7890 entries, 0 to 7889\n",
      "Data columns (total 21 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   rating                             7890 non-null   object \n",
      " 1   roaster                            7890 non-null   object \n",
      " 2   title                              7890 non-null   object \n",
      " 3   blind_assessment                   7889 non-null   object \n",
      " 4   notes                              7888 non-null   object \n",
      " 5   bottom_line                        3812 non-null   object \n",
      " 6   roaster location                   7887 non-null   object \n",
      " 7   coffee origin                      7386 non-null   object \n",
      " 8   roast level                        7488 non-null   object \n",
      " 9   agtron                             7890 non-null   object \n",
      " 10  est. price                         5852 non-null   object \n",
      " 11  review date                        7890 non-null   object \n",
      " 12  aroma                              7830 non-null   object \n",
      " 13  acidity                            3730 non-null   object \n",
      " 14  body                               7879 non-null   object \n",
      " 15  flavor                             7874 non-null   object \n",
      " 16  aftertaste                         7019 non-null   float64\n",
      " 17  url                                7890 non-null   object \n",
      " 18  acidity/structure                  2903 non-null   float64\n",
      " 19  with milk                          1118 non-null   object \n",
      " 20  refresh(enable javascript first.)  4 non-null      object \n",
      "dtypes: float64(2), object(19)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Read in raw data\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "df_in = pd.read_csv(DATA_DIR / \"raw\" / \"25072024_reviews.csv\")\n",
    "df_in.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Cleaning <a id='initial_cleaning'></a>\n",
    "[Back to top](#table_of_contents)\n",
    "\n",
    "First step is to do some of the basic data checks and cleanup. This includes dropping columns that are not needed, setting datatypes, renaming columns,\n",
    "combining columns, cleaning up strings, and creating new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7890 entries, 0 to 7889\n",
      "Data columns (total 21 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   rating                            7890 non-null   object \n",
      " 1   roaster                           7890 non-null   object \n",
      " 2   title                             7890 non-null   object \n",
      " 3   blind_assessment                  7889 non-null   object \n",
      " 4   notes                             7888 non-null   object \n",
      " 5   bottom_line                       3812 non-null   object \n",
      " 6   roaster_location                  7887 non-null   object \n",
      " 7   coffee_origin                     7386 non-null   object \n",
      " 8   roast_level                       7488 non-null   object \n",
      " 9   agtron                            7890 non-null   object \n",
      " 10  est_price                         5852 non-null   object \n",
      " 11  review_date                       7890 non-null   object \n",
      " 12  aroma                             7830 non-null   object \n",
      " 13  acidity                           3730 non-null   object \n",
      " 14  body                              7879 non-null   object \n",
      " 15  flavor                            7874 non-null   object \n",
      " 16  aftertaste                        7019 non-null   float64\n",
      " 17  url                               7890 non-null   object \n",
      " 18  acidity/structure                 2903 non-null   float64\n",
      " 19  with_milk                         1118 non-null   object \n",
      " 20  refresh(enable_javascript_first)  4 non-null      object \n",
      "dtypes: float64(2), object(19)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cleanup column names\n",
    "df_in.columns = (\n",
    "    df_in.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\".\", \"\")\n",
    ")\n",
    "\n",
    "df_in.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7566 entries, 0 to 7889\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   rating            7563 non-null   float64       \n",
      " 1   roaster           7566 non-null   object        \n",
      " 2   title             7566 non-null   object        \n",
      " 3   blind_assessment  7566 non-null   object        \n",
      " 4   notes             7565 non-null   object        \n",
      " 5   bottom_line       3758 non-null   object        \n",
      " 6   roaster_location  7564 non-null   object        \n",
      " 7   coffee_origin     7299 non-null   object        \n",
      " 8   roast_level       7445 non-null   object        \n",
      " 9   est_price         5790 non-null   object        \n",
      " 10  review_date       7566 non-null   datetime64[ns]\n",
      " 11  aroma             7540 non-null   float64       \n",
      " 12  acidity           6322 non-null   float64       \n",
      " 13  body              7556 non-null   float64       \n",
      " 14  flavor            7553 non-null   float64       \n",
      " 15  aftertaste        6954 non-null   float64       \n",
      " 16  url               7566 non-null   object        \n",
      " 17  with_milk         1082 non-null   object        \n",
      " 18  agtron_external   7566 non-null   float64       \n",
      " 19  agtron_ground     7566 non-null   float64       \n",
      " 20  is_espresso       7566 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(8), object(11)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "def tweak_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Initial data cleaning and feature creation\"\"\"\n",
    "    return (\n",
    "        df.assign(\n",
    "            review_date=lambda df_: pd.to_datetime(df_[\"review_date\"], format=\"%B %Y\"),\n",
    "            # Combing acidity and acidity/structure into one column, they are the same\n",
    "            # field but names used in reviews changed at one point\n",
    "            acidity=lambda df_: df_[\"acidity\"].fillna(df_[\"acidity/structure\"]),\n",
    "            # Split the agtron column into one for external bean agtron data and ground\n",
    "            # bean agtron data\n",
    "            agtron_external=lambda df_: pd.to_numeric(\n",
    "                df_[\"agtron\"].str.split(\"/\", expand=True)[0].str.strip(),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            agtron_ground=lambda df_: pd.to_numeric(\n",
    "                df_[\"agtron\"].str.split(\"/\", expand=True)[1].str.strip(),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            # Distinguish espresso roasts from other reviews\n",
    "            is_espresso=lambda df_: df_.apply(\n",
    "                lambda row: (\n",
    "                    True\n",
    "                    if \"espresso\" in row[\"title\"].lower()\n",
    "                    or pd.notnull(row[\"with_milk\"])\n",
    "                    else False\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "        )\n",
    "        .replace([\"\", \"NR\", \"N/A\", \"na\"], np.nan)\n",
    "        # Agtron values must be equalt to or below 100, some entries on website have typos\n",
    "        .loc[\n",
    "            lambda df_: (df_[\"agtron_external\"] <= 100) & (df_[\"agtron_ground\"] <= 100),\n",
    "            :,\n",
    "        ]\n",
    "        # Run str.strip on every string column\n",
    "        .map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        .drop(\n",
    "            columns=[\"acidity/structure\", \"agtron\", \"refresh(enable_javascript_first)\"]\n",
    "        )\n",
    "        .astype(\n",
    "            {\n",
    "                k: \"float\"\n",
    "                for k in [\n",
    "                    \"agtron_external\",\n",
    "                    \"agtron_ground\",\n",
    "                    \"acidity\",\n",
    "                    \"rating\",\n",
    "                    \"aroma\",\n",
    "                    \"body\",\n",
    "                    \"flavor\",\n",
    "                    \"aftertaste\",\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "df = tweak_df(df_in)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Price and Quantity Cleaning <a id='price_and_quantity_cleaning'></a>\n",
    "[Back to top](#table_of_contents)\n",
    "\n",
    "The `est_price` column contains information on price, currency, and quantity. We need to split this column up to separate the price and quantity information.\n",
    "\n",
    "Splitting on \"/\" character creates one column with price and currency information and another with quantity and unit of measurement information. \n",
    "\n",
    "The quantities have to be standardized so they contain a single representation for each unit and so unecessary punctuation and parentheses are removed.\n",
    "\n",
    "We also filter the dataset to remove all products that came in units of cans, boxes, capusles, pods, etc. We will only concern ourselves with coffee sold in bags or bulk, ground or whole.\n",
    "\n",
    "Regex is used to separate numerical and non-numerical characters from the quantity and price columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (7566, 21)\n",
      "Shape of price_quantity: (5610, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quantity_unit\n",
       "ounces                         4757\n",
       "grams                           823\n",
       "pounds                           17\n",
       "kg                                4\n",
       "kilogram                          4\n",
       "ounces (sold as a set of /$       3\n",
       "kilo                              1\n",
       "gram                              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_terms = [\n",
    "    \"can\",\n",
    "    \"box\",\n",
    "    \"capsules\",\n",
    "    \"K-\",\n",
    "    \"cups\",\n",
    "    \"bags\",\n",
    "    \"concentrate\",\n",
    "    \"discs\",\n",
    "    \"bottle\",\n",
    "    \"pods\",\n",
    "    \"ml\",\n",
    "    \"pods\",\n",
    "    \"pouch\",\n",
    "    \"packet|tin\",\n",
    "    \"instant\",\n",
    "    \"sachet\",\n",
    "    \"vue\",\n",
    "    \"single-serve\",\n",
    "    \"fluid\",\n",
    "    \"capsultes\",\n",
    "]\n",
    "\n",
    "drop_terms_string = \"|\".join(drop_terms)\n",
    "\n",
    "\n",
    "def price_quantity_split(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Original df shape: {df.shape}\")\n",
    "    price_quantity = (\n",
    "        df\n",
    "        # Split est_price into columns for price and quantity\n",
    "        .est_price.str.split(\"/\", n=1, expand=True)\n",
    "        # Remove any commas from the price and quantity columns\n",
    "        .replace(\",\", \"\", regex=True)\n",
    "        .rename(columns={0: \"price\", 1: \"quantity\"})\n",
    "        .assign(\n",
    "            # Cleanup quantity\n",
    "            quantity=lambda df_: (\n",
    "                df_[\"quantity\"]\n",
    "                .loc[\n",
    "            lambda df_: ~df_[\"quantity\"].str.contains(drop_terms_string, case=False),\n",
    "            :,\n",
    "        ]\n",
    "                .str.lower().str.strip()\n",
    "                # Remove parentheses and anything inside them\n",
    "                .str.replace(r\"\\(.*?\\)\", \"\", regex=True)\n",
    "                # Remove anything after a semicolon. This is usually a note, or deal price.\n",
    "                .str.replace(r\";.*\", \"\", regex=True)\n",
    "                # Standardize units\n",
    "                .str.replace(r\".g$\", \" grams\", regex=True)\n",
    "                .str.replace(r\"\\sg$\", \"grams\", regex=True)\n",
    "                .str.replace(r\"pound$\", \"1 pounds\", regex=True)\n",
    "                .str.replace(r\"oz|onces|ouncues|ounce$|ounces\\*\", \"ounces\", regex=True)\n",
    "                .str.replace(\"\\bkilo\\b\", \"kilograms\", regex=True)\n",
    "                # Remove \"online\" from any quantity\n",
    "                .str.replace(\"online\", \"\")\n",
    "                .str.strip()\n",
    "            ),\n",
    "            price=lambda df_: df_[\"price\"].str.replace(\"..\", \".\"),\n",
    "        )\n",
    "        .dropna()\n",
    "        # Split quantity into value and unit, and split price into value and currency\n",
    "        .assign(\n",
    "            # Extract number value from quantity\n",
    "            quantity_value=lambda df_: (\n",
    "                df_[\"quantity\"].str.extract(r\"(\\d+)\").astype(float)\n",
    "            ),\n",
    "            # Extract the unit from quantity column\n",
    "            quantity_unit=lambda df_: (\n",
    "                df_[\"quantity\"]\n",
    "                .str.replace(r\"(\\d+)\", \"\", regex=True)\n",
    "                .replace(\"\\.\", \"\", regex=True)\n",
    "                .str.strip()\n",
    "                .mask(lambda s: s == \"g\", \"grams\")\n",
    "                .str.strip()\n",
    "            ),\n",
    "            # Extract price value from price column\n",
    "            price_value=lambda df_: (\n",
    "                df_[\"price\"].str.extract(r\"(\\d+\\.\\d+|\\d+)\").astype(float)\n",
    "            ),\n",
    "            # Extract currency from price column\n",
    "            price_currency=lambda df_: (\n",
    "                df_[\"price\"]\n",
    "                .str.replace(\",\", \"\")\n",
    "                .str.replace(r\"(\\d+\\.\\d+|\\d+)\", \"\", regex=True)\n",
    "                .str.strip()\n",
    "            ),\n",
    "        )\n",
    "        # Drop the original price and quantity columns\n",
    "        .drop(columns=[\"price\", \"quantity\"])\n",
    "    )\n",
    "    print(f\"Shape of price_quantity: {price_quantity.shape}\")\n",
    "\n",
    "    # Merge the price_quantity DataFrame with the original DataFrame\n",
    "    return df.merge(price_quantity, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "df = df_in.pipe(tweak_df).pipe(price_quantity_split)\n",
    "\n",
    "df.quantity_unit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Currencies\n",
    "\n",
    "Normalize the currency column to contain a standardized set of currency symbols. We will use the ISO 4217 codes to make it easier to get foreign exchange data from an external API later on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (7566, 21)\n",
      "Shape of price_quantity: (5786, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>est_price</th>\n",
       "      <th>price_currency</th>\n",
       "      <th>price_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>AED $99.75/250 grams</td>\n",
       "      <td>AED</td>\n",
       "      <td>99.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>AED $99.75/250 grams</td>\n",
       "      <td>AED</td>\n",
       "      <td>99.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>AED $103.95/250 grams</td>\n",
       "      <td>AED</td>\n",
       "      <td>103.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>AUD $16.00/250 grams</td>\n",
       "      <td>AUD</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7640</th>\n",
       "      <td>AUD $48.00/200 grams</td>\n",
       "      <td>AUD</td>\n",
       "      <td>48.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>NT $750/8 ounces</td>\n",
       "      <td>TWD</td>\n",
       "      <td>750.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5958</th>\n",
       "      <td>NT $698/100 grams</td>\n",
       "      <td>TWD</td>\n",
       "      <td>698.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>$19.95/8 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>19.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>$20.00/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>$18.00/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  est_price price_currency  price_value\n",
       "7560   AED $99.75/250 grams            AED        99.75\n",
       "7560   AED $99.75/250 grams            AED        99.75\n",
       "5351  AED $103.95/250 grams            AED       103.95\n",
       "1030   AUD $16.00/250 grams            AUD        16.00\n",
       "7640   AUD $48.00/200 grams            AUD        48.00\n",
       "...                     ...            ...          ...\n",
       "3691       NT $750/8 ounces            TWD       750.00\n",
       "5958      NT $698/100 grams            TWD       698.00\n",
       "1404        $19.95/8 ounces            USD        19.95\n",
       "394        $20.00/12 ounces            USD        20.00\n",
       "4349       $18.00/12 ounces            USD        18.00\n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_currency(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize currencies to ISO 4217 codes.\"\"\"\n",
    "    price_currency = (\n",
    "        df.price_currency.str.upper()\n",
    "        .str.replace(r\"^\\$$\", \"USD\", regex=True)\n",
    "        .str.replace(\"PRICE: $\", \"USD\")\n",
    "        .str.replace(\"$\", \"\")\n",
    "        .str.replace(\"#\", \"GBP\")\n",
    "        .str.replace(\"¥\", \"JPY\")\n",
    "        .str.replace(\"£\", \"GBP\")\n",
    "        .str.replace(\"€\", \"EUR\")\n",
    "        .str.replace(\"POUND\", \"GBP\")\n",
    "        .str.replace(\"PESOS\", \"MXN\")\n",
    "        .str.replace(\"RMB\", \"CNY\")\n",
    "        .str.replace(\"EUROS\", \"EUR\")\n",
    "        .str.strip()\n",
    "        .mask(lambda s: s == \"US\", \"USD\")\n",
    "        .mask(lambda s: s == \" \", \"USD\")\n",
    "        .mask(lambda s: s == \"E\", \"EUR\")\n",
    "        .mask(lambda s: s == \"NTD\", \"TWD\")\n",
    "        .mask(lambda s: s == \"NT\", \"TWD\")\n",
    "        .mask(lambda s: s == \"\", \"USD\")\n",
    "        .mask(lambda s: s == \"HK\", \"HKD\")\n",
    "        .str.strip()\n",
    "    )\n",
    "    return df.assign(price_currency=price_currency)\n",
    "\n",
    "\n",
    "df = df_in.pipe(tweak_df).pipe(price_quantity_split).pipe(clean_currency)\n",
    "\n",
    "\n",
    "# Check that currencies make sense from original est_price column\n",
    "df.loc[:, [\"est_price\", \"price_currency\", \"price_value\"]].groupby(\n",
    "    \"price_currency\"\n",
    ").sample(3, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_currency\n",
       "USD    4267\n",
       "TWD    1068\n",
       "CAD     125\n",
       "HKD      46\n",
       "CNY      27\n",
       "THB      21\n",
       "KRW      20\n",
       "JPY      12\n",
       "GBP      10\n",
       "AUD      10\n",
       "EUR       6\n",
       "AED       5\n",
       "RM        3\n",
       "IDR       3\n",
       "GTQ       1\n",
       "MXN       1\n",
       "LAK       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.price_currency.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantity_unit\n",
       "ounces                         4757\n",
       "grams                           823\n",
       "pounds                           17\n",
       "kg                                4\n",
       "ounces (sold as a set of /$       3\n",
       "kilogram                          3\n",
       "kilo                              1\n",
       "gram                              1\n",
       "Kilogram                          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quantity_unit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting prices to 2024 USD\n",
    "\n",
    "1. Convert price to USD using historical exchange rates\n",
    "2. Adjust price to 2024 USD using BLS consumer price index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tylernardone/Projects/coffee-review/data/external/openex_exchange_rates.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Read in exchange rates\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexternal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenex_exchange_rates.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     18\u001b[0m     exchange_rates \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m     df_raw\u001b[38;5;241m.\u001b[39mpipe(tweak_df)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(price_quantity_split)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(clean_currency)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(convert_to_usd)\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/coffee-review/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tylernardone/Projects/coffee-review/data/external/openex_exchange_rates.json'"
     ]
    }
   ],
   "source": [
    "def convert_row(row):\n",
    "    date = row[\"review_date\"].strftime(\"%Y-%m-%d\")\n",
    "    currency = row[\"price_currency\"]\n",
    "    price = row[\"price_value\"]\n",
    "    if currency == \"USD\":\n",
    "        return price\n",
    "    else:\n",
    "        return np.round(price / exchange_rates[date][currency], 2)\n",
    "\n",
    "\n",
    "def convert_to_usd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"price_value_usd_hist\"] = df.apply(convert_row, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Read in exchange rates\n",
    "with open(DATA_DIR / \"external\" / \"openex_exchange_rates.json\") as f:\n",
    "    exchange_rates = json.load(f)\n",
    "\n",
    "df = (\n",
    "    df_in.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    ")\n",
    "\n",
    "(\n",
    "    df.loc[:, [\"price_value\", \"price_currency\", \"price_value_usd_hist\"]].groupby(\n",
    "        \"price_currency\"\n",
    "    )\n",
    ").sample(3, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_cpi(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads and transforms the CPI data.\"\"\"\n",
    "    MONTH_MAP = {\n",
    "        \"Jan\": 1,\n",
    "        \"Feb\": 2,\n",
    "        \"Mar\": 3,\n",
    "        \"Apr\": 4,\n",
    "        \"May\": 5,\n",
    "        \"Jun\": 6,\n",
    "        \"Jul\": 7,\n",
    "        \"Aug\": 8,\n",
    "        \"Sep\": 9,\n",
    "        \"Oct\": 10,\n",
    "        \"Nov\": 11,\n",
    "        \"Dec\": 12,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        cpi = pd.read_csv(\n",
    "            file_path,\n",
    "            usecols=[\n",
    "                \"Year\",\n",
    "                \"Jan\",\n",
    "                \"Feb\",\n",
    "                \"Mar\",\n",
    "                \"Apr\",\n",
    "                \"May\",\n",
    "                \"Jun\",\n",
    "                \"Jul\",\n",
    "                \"Aug\",\n",
    "                \"Sep\",\n",
    "                \"Oct\",\n",
    "                \"Nov\",\n",
    "                \"Dec\",\n",
    "            ],\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"CPI file is not found in the specified directory.\")\n",
    "\n",
    "    return (\n",
    "        cpi.melt(id_vars=\"Year\", var_name=\"Month\", value_name=\"cpi\")\n",
    "        .assign(\n",
    "            Month=lambda x: x[\"Month\"].map(MONTH_MAP),\n",
    "            date=lambda x: pd.to_datetime(x[[\"Year\", \"Month\"]].assign(day=1)),\n",
    "        )\n",
    "        .dropna()\n",
    "        .drop(columns=[\"Year\", \"Month\"])\n",
    "        .rename(columns={\"cpi\": \"consumer_price_index\"})\n",
    "        .sort_values(\"date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cpi_adjusted_price(\n",
    "    df: pd.DataFrame, file_path: Path, date: str = \"2024-01-01\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adjusts historical price data to 2024 prices using CPI data.\n",
    "    \"\"\"\n",
    "    cpi = load_transform_cpi(file_path)\n",
    "    cpi_baseline = cpi.loc[cpi[\"date\"] == date, \"consumer_price_index\"].values[0]\n",
    "\n",
    "    return (\n",
    "        df.merge(cpi, left_on=\"review_date\", right_on=\"date\")\n",
    "        .drop(columns=\"date\")\n",
    "        .assign(\n",
    "            price_usd_adj_2024=lambda df_: np.round(\n",
    "                df_[\"price_value_usd_hist\"]\n",
    "                * cpi_baseline\n",
    "                / df_[\"consumer_price_index\"],\n",
    "                2,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "data_dir = Path(\"../../data\")\n",
    "cpi_path = data_dir / \"external\" / \"consumer_price_index.csv\"\n",
    "\n",
    "df = (\n",
    "    df_raw.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    "    .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    ")\n",
    "\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.assign(\n",
    "        price_diff=lambda df_: (df_[\"price_usd_adj_2024\"] - df_[\"price_value_usd_hist\"])\n",
    "        / df_[\"price_usd_adj_2024\"]\n",
    "    )\n",
    ").plot(\n",
    "    x=\"review_date\",\n",
    "    y=\"price_diff\",\n",
    "    title=\"% Price difference between adjusted and historical prices\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting quantities to lbs\n",
    "\n",
    "Create a normalized quantity column that converts all quantities to lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lbs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    to_lbs_conversion = {\n",
    "        \"ounces\": 1 / 16,\n",
    "        \"pounds\": 1,\n",
    "        \"kilogram\": 2.20462,\n",
    "        \"grams\": 0.00220462,\n",
    "    }\n",
    "    df[\"quantity_in_lbs\"] = np.round(\n",
    "        df[\"quantity_value\"] * df[\"quantity_unit\"].map(to_lbs_conversion), 2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = (\n",
    "    df_raw.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    "    .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "    .pipe(convert_to_lbs)\n",
    ")\n",
    "\n",
    "(\n",
    "    df.loc[:, [\"quantity_value\", \"quantity_unit\", \"quantity_in_lbs\"]]\n",
    "    .groupby(\"quantity_unit\")\n",
    "    .sample(3, replace=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for price per pound\n",
    "def price_per_lbs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"price_usd_adj_2024_per_lb\"] = np.round(\n",
    "        df[\"price_usd_adj_2024\"] / df[\"quantity_in_lbs\"], 2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = (\n",
    "    df_raw.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    "    .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "    .pipe(convert_to_lbs)\n",
    "    .pipe(price_per_lbs)\n",
    ")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of US states\n",
    "us_states_and_territories = [\n",
    "    \"Alabama\",\n",
    "    \"Alaska\",\n",
    "    \"Arizona\",\n",
    "    \"Arkansas\",\n",
    "    \"California\",\n",
    "    \"Colorado\",\n",
    "    \"Connecticut\",\n",
    "    \"Delaware\",\n",
    "    \"Florida\",\n",
    "    \"Georgia\",\n",
    "    \"Hawaii\",\n",
    "    \"Idaho\",\n",
    "    \"Illinois\",\n",
    "    \"Indiana\",\n",
    "    \"Iowa\",\n",
    "    \"Kansas\",\n",
    "    \"Kentucky\",\n",
    "    \"Louisiana\",\n",
    "    \"Maine\",\n",
    "    \"Maryland\",\n",
    "    \"Massachusetts\",\n",
    "    \"Michigan\",\n",
    "    \"Minnesota\",\n",
    "    \"Mississippi\",\n",
    "    \"Missouri\",\n",
    "    \"Montana\",\n",
    "    \"Nebraska\",\n",
    "    \"Nevada\",\n",
    "    \"New Hampshire\",\n",
    "    \"New Jersey\",\n",
    "    \"New Mexico\",\n",
    "    \"New York\",\n",
    "    \"North Carolina\",\n",
    "    \"North Dakota\",\n",
    "    \"Ohio\",\n",
    "    \"Oklahoma\",\n",
    "    \"Oregon\",\n",
    "    \"Pennsylvania\",\n",
    "    \"Rhode Island\",\n",
    "    \"South Carolina\",\n",
    "    \"South Dakota\",\n",
    "    \"Tennessee\",\n",
    "    \"Texas\",\n",
    "    \"Utah\",\n",
    "    \"Vermont\",\n",
    "    \"Virginia\",\n",
    "    \"Washington\",\n",
    "    \"West Virginia\",\n",
    "    \"Wisconsin\",\n",
    "    \"Wyoming\",\n",
    "    \"District of Columbia\",\n",
    "    \"Puerto Rico\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_us_state(row):\n",
    "    if row[\"territorial_entity_2\"] in us_states_and_territories:\n",
    "        return row[\"territorial_entity_2\"]\n",
    "    elif row[\"territorial_entity_1\"] in us_states_and_territories:\n",
    "        return row[\"territorial_entity_1\"]\n",
    "    elif row[\"og_roaster_location\"].split(\",\")[-1].strip() in us_states_and_territories:\n",
    "        return row[\"og_roaster_location\"].split(\",\")[-1].strip()\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def create_county_and_state_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"roaster_county\"] = np.where(\n",
    "        df[\"territorial_entity_1\"].str.contains(\"County\", na=False),\n",
    "        df[\"territorial_entity_1\"],\n",
    "        np.nan,\n",
    "    )\n",
    "    df[\"roaster_us_state\"] = df.apply(create_us_state, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = (\n",
    "    df_raw.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    "    .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "    .pipe(convert_to_lbs)\n",
    "    .pipe(price_per_lbs)\n",
    "    .pipe(create_county_and_state_columns)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df.loc[\n",
    "        df[\"roaster_country\"] == \"USA\",\n",
    "        [\"roaster_country\", \"roaster_us_state\", \"roaster_county\"],\n",
    "    ].info()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Checks <a id='data_checks'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df_raw.pipe(tweak_df)\n",
    "    .pipe(price_quantity_split)\n",
    "    .pipe(clean_currency)\n",
    "    .pipe(convert_to_usd)\n",
    "    .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "    .pipe(convert_to_lbs)\n",
    "    .pipe(price_per_lbs)\n",
    "    .pipe(create_county_and_state_columns)\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=[\"number\"]).drop(columns=[\"price_value\"], axis=1)\n",
    "len(df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(df_numeric.columns):\n",
    "    df[col].plot(\n",
    "        kind=\"hist\",\n",
    "        ax=ax[i // 5, i % 5],\n",
    "        title=col,\n",
    "        bins=20,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"price_usd_adj_2024_per_lb\"] < 200].price_usd_adj_2024_per_lb.hist(\n",
    "    cumulative=True, density=True, edgecolor=\"black\", alpha=0.7\n",
    ")\n",
    "plt.title(\"Cumulative Histogram Price $USD/lbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking countries\n",
    "display(df.roaster_country.sort_values().unique())\n",
    "display(df.coffee_origin_country.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking roast level\n",
    "df.roast_level.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Export cleaned data <a id='export_data'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = data_dir / \"processed\" / \"05052024_roast_review_cleaned.csv\"\n",
    "df.to_csv(fout, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee-review-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
